{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Object detection in videos with DETR\n",
    "\n",
    "Now that we've seen how we can get some nice results for images, let's see if we can generalise this to video. "
   ],
   "id": "b71e278352efb750"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T13:43:51.622768Z",
     "start_time": "2024-11-04T13:43:50.309262Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import DetrImageProcessor, DetrForObjectDetection \n",
    "import torch\n",
    "import cv2\n",
    "\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import numpy as np\n",
    "import time"
   ],
   "id": "ac4db5e6a05c3cfd",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Again, we load in the DETR model using the Transformer package.",
   "id": "e5ff7a20f84f330e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T13:43:54.725326Z",
     "start_time": "2024-11-04T13:43:53.585657Z"
    }
   },
   "cell_type": "code",
   "source": [
    "processor = DetrImageProcessor.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\") \n",
    "model = DetrForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", revision=\"no_timm\")"
   ],
   "id": "45ad38845b502c41",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We use the same convenience functions to create the label font size and colour of the bounding boxes.",
   "id": "8b26487225176cc0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T13:43:56.646281Z",
     "start_time": "2024-11-04T13:43:56.641556Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def calculate_label_size(box_width, box_height, frame_width, frame_height, min_scale=0.4, max_scale=1.2):\n",
    "    \"\"\"\n",
    "    Calculate appropriate font scale based on bounding box size and frame dimensions.\n",
    "    \"\"\"\n",
    "    box_size_ratio = (box_width * box_height) / (frame_width * frame_height)\n",
    "    font_scale = min_scale + (np.log(1 + box_size_ratio * 100) / 5) * (max_scale - min_scale)\n",
    "    return np.clip(font_scale, min_scale, max_scale)\n",
    "\n",
    "def generate_color_palette(num_classes):\n",
    "    \"\"\"\n",
    "    Generate a separate colour for every label predicted for that image. \n",
    "    \"\"\"\n",
    "    np.random.seed(42)  # Ensure the same colors are generated every time\n",
    "    return {label: tuple(np.random.randint(0, 255, 3).tolist()) for label in range(num_classes)}\n",
    "\n",
    "# Create a color palette for each class (COCO has 91 classes by default)\n",
    "colors = generate_color_palette(num_classes=91)\n",
    "\n",
    "# Function to get the color for a specific label\n",
    "def get_label_color(label):\n",
    "    return colors.get(label, (0, 255, 0))  # Default to green if no label found"
   ],
   "id": "7d59f82e060b1626",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can keep track of the frames-per-second using an FPS counter. The function below contains all the formatting option we need to make a nicely presented counter in the top corner of the video.  ",
   "id": "c8fbd47092d9be7f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T13:44:01.778072Z",
     "start_time": "2024-11-04T13:44:01.774434Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def add_fps_counter(frame, fps, frame_width):\n",
    "    \"\"\"\n",
    "    Add FPS counter to the frame.\n",
    "\n",
    "    Args:\n",
    "        frame: The video frame\n",
    "        fps: Current FPS\n",
    "        frame_width: Width of the frame\n",
    "    \"\"\"\n",
    "    # Format FPS text\n",
    "    fps_text = f\"FPS: {fps:.1f}\"\n",
    "\n",
    "    # Set font properties\n",
    "    font_scale = 0.7\n",
    "    font_thickness = 2\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "    # Get text size\n",
    "    (text_width, text_height), baseline = cv2.getTextSize(\n",
    "        fps_text, font, font_scale, font_thickness\n",
    "    )\n",
    "\n",
    "    # Calculate position (top right corner with padding)\n",
    "    padding = 10\n",
    "    x = frame_width - text_width - padding\n",
    "    y = text_height + padding + baseline\n",
    "\n",
    "    # Draw background rectangle\n",
    "    cv2.rectangle(\n",
    "        frame,\n",
    "        (int(x - 5), int(y - text_height - baseline - 5)),\n",
    "        (int(x + text_width + 5), int(y + 5)),\n",
    "        (0, 0, 0),\n",
    "        -1\n",
    "    )\n",
    "\n",
    "    # Draw FPS text\n",
    "    cv2.putText(\n",
    "        frame,\n",
    "        fps_text,\n",
    "        (int(x), int(y)),\n",
    "        font,\n",
    "        font_scale,\n",
    "        (255, 255, 255),  # White text\n",
    "        font_thickness,\n",
    "        cv2.LINE_AA\n",
    "    )"
   ],
   "id": "9f70c13b5c410744",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Finally, we're ready to run inference over the video. The function below does the following:\n",
    "* Detects whether a GPU accelerator is available to apply inference to each frame. In my case, I have MPS (MacBook M3), so we'll enable this.\n",
    "* OpenCVs `VideoCapture` method is used to open a connection to the video.\n",
    "* The number of frames in the video is calculated, as well as the size of each one.\n",
    "* The number of frames is used to calculate the FPS and create the progress bar.\n",
    "* For each frame:\n",
    "    * The image is converted from BGR to RGB, and converted to tensors\n",
    "    * The image is input into the DETR model for inference\n",
    "    * The result is post-processed, extracting the most likely label, as well as its probability and bounding box coordinates\n",
    "    * And then for each detected bounding box ...\n",
    "        * The results are plotted, with their associated label and class colour\n",
    "* At the last frame, the connection to the video is closed."
   ],
   "id": "54d6d527ab0dfe19"
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-11-04T13:44:05.232319Z",
     "start_time": "2024-11-04T13:44:05.222674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def process_video(video_path, output_path, threshold = 0.9):\n",
    "    \"\"\"\n",
    "    Process a video file using GPU acceleration (if available) for object detection.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        raise ValueError(\"Error opening video file\")\n",
    "\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_path, fourcc, fps, (frame_width, frame_height))\n",
    "\n",
    "    pbar = tqdm(total=total_frames, desc=\"Processing frames\",\n",
    "                unit=\"frames\", dynamic_ncols=True)\n",
    "\n",
    "    frame_count = 0 # To count total frames.\n",
    "    total_fps = 0 # To get the final frames per second.\n",
    "\n",
    "    try:\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "\n",
    "            # Start timing for this frame\n",
    "            start_time = time.time()\n",
    "\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            inputs = processor(images=frame_rgb, return_tensors=\"pt\")\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**inputs)\n",
    "\n",
    "            target_sizes = torch.tensor([frame_rgb.shape[:2]]).to(device)\n",
    "            results = processor.post_process_object_detection(outputs, target_sizes=target_sizes)[0]\n",
    "\n",
    "            scores = results[\"scores\"].cpu().numpy()\n",
    "            keep = scores > threshold\n",
    "\n",
    "            boxes = results[\"boxes\"].cpu().numpy()[keep]\n",
    "            labels = results[\"labels\"].cpu().numpy()[keep]\n",
    "            scores = scores[keep]\n",
    "\n",
    "            # Draw bounding boxes\n",
    "            for box, label, score in zip(boxes, labels, scores):\n",
    "                xmin, ymin, xmax, ymax = box\n",
    "                box_width = xmax - xmin\n",
    "                box_height = ymax - ymin\n",
    "                font_scale = calculate_label_size(box_width, box_height, frame_width, frame_height)\n",
    "\n",
    "                label_text = f\"{model.config.id2label[label]}: {score:.2f}\"\n",
    "                (text_width, text_height), baseline = cv2.getTextSize(\n",
    "                    label_text,\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    font_scale,\n",
    "                    1\n",
    "                )\n",
    "\n",
    "                # Get the color for this label\n",
    "                color = get_label_color(label)\n",
    "\n",
    "                # Draw rectangle and label with the same color for the same class\n",
    "                cv2.rectangle(frame, (int(xmin), int(ymin)), (int(xmax), int(ymax)), color, max(2, int(font_scale * 3)))\n",
    "                cv2.rectangle(frame, (int(xmin), int(ymin) - text_height - baseline - 5),\n",
    "                              (int(xmin) + text_width, int(ymin)), color, -1)\n",
    "                cv2.putText(frame, label_text, (int(xmin), int(ymin) - baseline - 2),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, font_scale, (0, 0, 0), max(1, int(font_scale * 1.5)), cv2.LINE_AA)\n",
    "\n",
    "            end_time = time.time()\n",
    "            # Get the current fps.\n",
    "            fps = 1 / (end_time - start_time)\n",
    "            # Add `fps` to `total_fps`.\n",
    "            total_fps += fps\n",
    "            # Increment frame count.\n",
    "            frame_count += 1\n",
    "\n",
    "            # Add FPS counter to frame\n",
    "            add_fps_counter(frame, fps, frame_width)\n",
    "\n",
    "            out.write(frame)\n",
    "            pbar.update(1)\n",
    "\n",
    "            if pbar.n % 100 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        raise\n",
    "    finally:\n",
    "        pbar.close()\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        model.to('cpu')\n",
    "\n",
    "    print(\"\\nVideo processing completed!\")"
   ],
   "id": "initial_id",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Let's apply this to a video!\n",
    "\n",
    "We'll use another video from one of my holidays: a little clip of driving along Tower Bridge in London. This scene has a lot going on so it will give us a good idea of what DETR is capable of."
   ],
   "id": "1850591ed52bad8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T13:44:09.504316Z",
     "start_time": "2024-11-04T13:44:09.500924Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def display_mp4(video_path):\n",
    "\n",
    "    # Create a VideoCapture object and read from input file\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    \n",
    "    # Check if camera opened successfully\n",
    "    if (cap.isOpened() == False):\n",
    "        print(\"Error opening video file\")\n",
    "    \n",
    "    # Read until video is completed\n",
    "    while (cap.isOpened()):\n",
    "    \n",
    "        # Capture frame-by-frame\n",
    "        ret, frame = cap.read()\n",
    "        if ret == True:\n",
    "            # Display the resulting frame\n",
    "            cv2.imshow('Frame', frame)\n",
    "    \n",
    "            # Press Q on keyboard to exit\n",
    "            if cv2.waitKey(25) & 0xFF == ord('q'):\n",
    "                break\n",
    "    \n",
    "        # Break the loop\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    # When everything done, release\n",
    "    # the video capture object\n",
    "    cap.release()\n",
    "    \n",
    "    # Closes all the frames\n",
    "    cv2.destroyAllWindows()"
   ],
   "id": "3cc55c0c51a46d8f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T13:46:56.412619Z",
     "start_time": "2024-11-04T13:45:34.826864Z"
    }
   },
   "cell_type": "code",
   "source": "display_mp4(\"inference_data/own_videos/tower-bridge-london-long.mp4\")",
   "id": "b653161361b82463",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-04 14:45:34.933 Python[97355:36126189] WARNING: Secure coding is not enabled for restorable state! Enable secure coding by implementing NSApplicationDelegate.applicationSupportsSecureRestorableState: and returning YES.\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T13:48:13.971560Z",
     "start_time": "2024-11-04T13:47:13.926989Z"
    }
   },
   "cell_type": "code",
   "source": "process_video(\"inference_data/own_videos/tower-bridge-london.mp4\", \"tower-bridge-labeled.mp4\")",
   "id": "918d3e6f41924655",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing frames: 100%|██████████| 664/664 [00:59<00:00, 11.12frames/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Video processing completed!\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-04T13:47:09.230381Z",
     "start_time": "2024-11-04T13:47:09.226905Z"
    }
   },
   "cell_type": "code",
   "source": "display_mp4(\"inference_data/own_videos/tower-bridge-labeled.mp4\")",
   "id": "e8cac9890ec51c57",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error opening video file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"inference_data/own_videos/tower-bridge-labeled.mp4\"\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4a6517d0b16a8062"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
